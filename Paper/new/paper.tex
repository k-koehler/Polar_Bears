\title{Predicting Polar Bear Management Zones from Isotopic Features}
\author{Kevin Koehler}
\date{\today}

\documentclass[12pt]{article}

\usepackage{amsmath, amsfonts}

\begin{document}
\maketitle

\begin{abstract}

We use various machine learning classifiers on isotopic composition features $\delta2H, \delta13C, \delta15N,$ and $\delta18O$ to identify polar bear management zones. 

\end{abstract}

\section{Models}

\subsection{Bayesian Classifiers}

\subsubsection{Theory}

Bayesian classifiers estimate the  probability of a class label given a vector of features. Let $\mathbb{L} = \{ l_1, ..., l_k \}$ be the set of class labels and each feature vector $\mathbf{x}$ be in $\mathbb{R}^n$, then the \textit{maximum a posteriori (MAP)} classifier assigns the following label $\hat{y}$ to $\mathbf{x}$:

$$
\hat{y} = \max_{l_i \in \mathbb{L}} P(l_i|x_1, ..., x_n)
$$

A na誰ve Bayesan classifier reduces $P(l_i|x_1, ..., x_n)$ to $P(l_i)\prod_{x_j \in \mathbf{x}}P(x_j|l_i)$ by making the \textit{na誰ve assumption} whereby we assume conditional independence between all $x_p, x_q \in \mathbf{x}$ where $p \neq q$. To see this, first the apply the chain rule to joint probability (which is proportional to the overall expression):
\begin{align*}
&P(c_i, x_1, ..., x_n) \\
= &P(x_1|x_2,..., x_n,l_i)...P(x_{n-1},x_n|l_i)P(x_n|l_i)P(l_i)
\end{align*}

With the na誰ve assumption we have $P(x_j|x_{j+1}, ..., x_n, l_i = P(x_j|l_i)$ thus every $P(x_j|x_{j+1},...,x_n, l_i) = P(x_j|l_i)$ yielding:

\begin{align*}
\hat{y} &= \prod_{x_j \in \mathbf{x}}P(x_j|l_i)P(l_i) \\
&= P(l_i)\prod_{x_j \in \mathbf{x}}P(x_j|l_i)
\end{align*}

\subsubsection{Results}

As is typical for problems of this nature, the na誰ve Bayesian classifier is surprisingly accurate. As one-hot encoding the features is not useful for Bayesian classifiers, the only pre-processing that was done was dropping incomplete feature vectors, as well as PCA, which had a negligible effect.

Since there is some randomness, the accuracy supplied is an average of 10 simulations. The accuracy for this model had a mean of 0.76281 with a variance of 0.0965.

For more advanced metrics, see the \textbf{Discussion} section of this paper.

\subsection{Support Vector Machine}

\end{document}
